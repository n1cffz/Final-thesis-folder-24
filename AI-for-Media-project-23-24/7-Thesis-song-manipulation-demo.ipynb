{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair the Emoton Prediction Model with Musical Logic\n",
    "- Same code from '7-Thesis-song-choice-demo.ipynb' but debugged using GPT when implementing softmax and buffering, when mismatch between the expected input shape of your CNN model prevented the camera working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import dlib\n",
    "import joblib\n",
    "import pygame\n",
    "from collections import deque\n",
    "from keras.models import load_model\n",
    "from scipy.special import softmax\n",
    "import librosa\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "CNN Predictions: [[0.00254196 0.0012075  0.08095461 0.9152959 ]]\n",
      "CNN Probabilities: [[0.17948781 0.17924845 0.19412844 0.4471353 ]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "CNN Predictions: [[2.1227135e-03 5.3287687e-04 1.7405245e-02 9.7993910e-01]]\n",
      "CNN Probabilities: [[0.17629048 0.17601043 0.17900534 0.46869373]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "CNN Predictions: [[0.00707293 0.00363335 0.26454356 0.7247501 ]]\n",
      "CNN Probabilities: [[0.18726988 0.18662687 0.24226241 0.38384083]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "CNN Predictions: [[0.00152109 0.00344019 0.9498646  0.04517412]]\n",
      "CNN Probabilities: [[0.17768393 0.17802526 0.45867866 0.18561217]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "CNN Predictions: [[0.00297935 0.00500751 0.7836868  0.20832628]]\n",
      "CNN Probabilities: [[0.18474053 0.18511559 0.40329155 0.22685231]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "CNN Predictions: [[0.0035599  0.00480014 0.75019455 0.24144542]]\n",
      "CNN Probabilities: [[0.18588424 0.1861149  0.39219484 0.23580603]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "CNN Predictions: [[0.00257122 0.00452895 0.881349   0.1115508 ]]\n",
      "CNN Probabilities: [[0.18099369 0.18134837 0.43582463 0.20183323]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "CNN Predictions: [[0.00365079 0.00458231 0.683057   0.30870992]]\n",
      "CNN Probabilities: [[0.18760517 0.18778002 0.37008986 0.25452495]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "CNN Predictions: [[7.4362819e-04 2.9595231e-03 9.7032410e-01 2.5972741e-02]]\n",
      "CNN Probabilities: [[0.17653479 0.17692639 0.4654936  0.18104523]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "CNN Predictions: [[0.00368662 0.00351043 0.25059617 0.74220675]]\n",
      "CNN Probabilities: [[0.18612534 0.18609256 0.23825224 0.38952985]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "CNN Predictions: [[0.00315741 0.00393511 0.52602834 0.46687913]]\n",
      "CNN Probabilities: [[0.18947928 0.1896267  0.31962568 0.30126834]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "CNN Predictions: [[0.00198814 0.00411026 0.962642   0.03125966]]\n",
      "CNN Probabilities: [[0.1771406  0.17751691 0.46294007 0.18240239]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "CNN Predictions: [[5.6655495e-04 1.9139128e-03 9.8398340e-01 1.3536187e-02]]\n",
      "CNN Probabilities: [[0.17580938 0.17604642 0.47003973 0.17810442]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "CNN Predictions: [[0.00280997 0.00450585 0.73423076 0.25845346]]\n",
      "CNN Probabilities: [[0.18618436 0.18650039 0.38689685 0.24041842]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "CNN Predictions: [[9.4249315e-04 3.2132270e-03 9.6563315e-01 3.0211125e-02]]\n",
      "CNN Probabilities: [[0.17680494 0.17720687 0.46393192 0.18205626]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "CNN Predictions: [[0.00283086 0.00476879 0.82269657 0.16970375]]\n",
      "CNN Probabilities: [[0.18336046 0.18371613 0.41626397 0.21665944]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "CNN Predictions: [[0.00153994 0.00368165 0.9589506  0.03582787]]\n",
      "CNN Probabilities: [[0.1772431  0.17762311 0.4617081  0.18342578]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load pre-trained models\n",
    "rf_model = joblib.load('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/random_forest_model.pkl')\n",
    "scaler = joblib.load('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/scaler.pkl')\n",
    "cnn_model = load_model('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/model.h5')\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/haar/haarcascade_frontalface_alt2.xml')\n",
    "if face_cascade.empty():\n",
    "    raise IOError(\"Failed to load Haar Cascade file. Please check the file path.\")\n",
    "\n",
    "# Load Dlib's shape predictor for facial landmarks\n",
    "shape_predictor = dlib.shape_predictor('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define emotions map\n",
    "emotion_map = {0: 'angry', 1: 'happy', 2: 'sad', 3: 'neutral'}\n",
    "\n",
    "# Emotion buffer for smoothing predictions\n",
    "emotion_buffer = deque(maxlen=20)  # Keep last 20 frames\n",
    "\n",
    "# Path to neutral song\n",
    "neutral_song_path = 'Music/neutral/aklim_hep_sende.mp3'\n",
    "\n",
    "# Load and play the initial song\n",
    "pygame.mixer.music.load(neutral_song_path)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "# Function to manipulate audio based on emotion\n",
    "def manipulate_audio(file_path, emotion):\n",
    "    y, sr = librosa.load(file_path)\n",
    "    \n",
    "    # Adjust tempo based on the emotional states\n",
    "    if emotion == 'happy':\n",
    "        y_fast = librosa.effects.time_stretch(y, rate=1.5)\n",
    "    elif emotion == 'sad':\n",
    "        y_fast = librosa.effects.time_stretch(y, rate=0.9)\n",
    "    elif emotion == 'angry':\n",
    "        y_fast = librosa.effects.time_stretch(y, rate=1.5)\n",
    "    else:  # Neutral\n",
    "        y_fast = librosa.effects.time_stretch(y, rate=1.0)\n",
    "\n",
    "    # Adjust pitch and apply additional effects for \"angry\"\n",
    "    if emotion == 'angry':\n",
    "        y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=5)\n",
    "        y_shifted = np.tanh(y_shifted * 15)\n",
    "        y_shifted = librosa.effects.preemphasis(y_shifted, coef=1)\n",
    "    elif emotion == 'sad':\n",
    "        y_shifted = librosa.effects.pitch_shift(y_fast, sr=sr, n_steps=-4)\n",
    "    else:\n",
    "        y_shifted = y_fast\n",
    "\n",
    "    return y_shifted, sr\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)  # Try changing the index if you have multiple cameras\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video capture.\")\n",
    "    exit()\n",
    "\n",
    "current_emotion = 'neutral'\n",
    "prev_emotion = 'neutral'\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw bounding box around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
    "\n",
    "        # Get the landmarks\n",
    "        dlib_rect = dlib.rectangle(x, y, x + w, y + h)\n",
    "        landmarks = shape_predictor(gray, dlib_rect)\n",
    "\n",
    "        # Extract facial features for Random Forest\n",
    "        left_eye = np.array([landmarks.part(36).x, landmarks.part(36).y])\n",
    "        right_eye = np.array([landmarks.part(45).x, landmarks.part(45).y])\n",
    "        nose_tip = np.array([landmarks.part(30).x, landmarks.part(30).y])\n",
    "        mouth_left = np.array([landmarks.part(48).x, landmarks.part(48).y])\n",
    "        mouth_right = np.array([landmarks.part(54).x, landmarks.part(54).y])\n",
    "        chin = np.array([landmarks.part(8).x, landmarks.part(8).y])\n",
    "        left_eyebrow = np.array([landmarks.part(21).x, landmarks.part(21).y])\n",
    "        right_eyebrow = np.array([landmarks.part(22).x, landmarks.part(22).y])\n",
    "        \n",
    "        # Calculate distances and angles (features used during training)\n",
    "        total_face_area = w * h\n",
    "        left_eyebrow_angle = np.arctan2(left_eyebrow[1] - left_eye[1], left_eyebrow[0] - left_eye[0])\n",
    "        right_eyebrow_angle = np.arctan2(right_eyebrow[1] - right_eye[1], right_eyebrow[0] - right_eye[0])\n",
    "        eyebrow_distance = np.linalg.norm(left_eyebrow - right_eyebrow)\n",
    "        eyes_dist = np.linalg.norm(left_eye - right_eye)\n",
    "        eyes_to_nose_dist = np.linalg.norm((left_eye + right_eye) / 2 - nose_tip)\n",
    "        nose_to_mouth_dist = np.linalg.norm(nose_tip - (mouth_left + mouth_right) / 2)\n",
    "        mouth_angle = np.arctan2(mouth_right[1] - mouth_left[1], mouth_right[0] - mouth_left[0])\n",
    "        nose_angle = np.arctan2(nose_tip[1] - chin[1], nose_tip[0] - chin[0])\n",
    "\n",
    "        # Prepare the feature vector for Random Forest\n",
    "        features = np.array([[total_face_area, left_eyebrow_angle, right_eyebrow_angle, eyebrow_distance, \n",
    "                              eyes_dist, eyes_to_nose_dist, nose_to_mouth_dist, mouth_angle, nose_angle]])\n",
    "\n",
    "        # Use the original feature names used during fitting\n",
    "        original_feature_names = ['TotalFaceArea', 'LeftEyebrowAngle', 'RightEyebrowAngle', 'EyebrowDistance', \n",
    "                                  'EyesDist', 'EyesToNoseDist', 'NoseToMouthDist', 'MouthAngle', 'NoseAngle']\n",
    "        features_df = pd.DataFrame(features, columns=original_feature_names)\n",
    "\n",
    "        # Scale the features\n",
    "        features_scaled = scaler.transform(features_df)\n",
    "\n",
    "        # Predict the emotion with Random Forest\n",
    "        rf_emotion_label = rf_model.predict(features_scaled)[0]\n",
    "        rf_emotion_text = emotion_map.get(rf_emotion_label, \"Unknown\")\n",
    "\n",
    "        # Prepare the image for CNN (150x150 as per your earlier configuration)\n",
    "        face_img = cv2.resize(frame[y:y+h, x:x+w], (150, 150))\n",
    "        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "        face_img = face_img / 255.0  # Normalize to [0, 1]\n",
    "        face_img = np.expand_dims(face_img, axis=-1)  # Add channel dimension for grayscale\n",
    "        face_img = np.expand_dims(face_img, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict the emotion with CNN\n",
    "        cnn_predictions = cnn_model.predict(face_img)\n",
    "        print(\"CNN Predictions:\", cnn_predictions)  # Debugging: Print raw predictions\n",
    "        cnn_probabilities = softmax(cnn_predictions, axis=1)  # Apply softmax to get probabilities\n",
    "        print(\"CNN Probabilities:\", cnn_probabilities)  # Debugging: Print probabilities\n",
    "        cnn_emotion_label = np.argmax(cnn_probabilities, axis=1)[0]\n",
    "        cnn_emotion_text = emotion_map.get(cnn_emotion_label, \"Unknown\")\n",
    "\n",
    "        # Combine the predictions using weighted voting\n",
    "        if rf_emotion_text == cnn_emotion_text:\n",
    "            final_emotion_text = rf_emotion_text\n",
    "        else:\n",
    "            # Use softmax probabilities to weigh the decision\n",
    "            rf_weight = 0.5  # Example weight for Random Forest\n",
    "            cnn_weight = cnn_probabilities[0][cnn_emotion_label]  # Use the probability of the predicted class\n",
    "            if cnn_weight > rf_weight:\n",
    "                final_emotion_text = cnn_emotion_text\n",
    "            else:\n",
    "                final_emotion_text = rf_emotion_text\n",
    "\n",
    "        # Display the emotion on the frame\n",
    "        cv2.putText(frame, final_emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "        # Add the predicted emotion to the buffer\n",
    "        emotion_buffer.append(final_emotion_text)\n",
    "\n",
    "        # Get the most common emotion in the buffer\n",
    "        common_emotion = max(set(emotion_buffer), key=emotion_buffer.count)\n",
    "\n",
    "        # Change the music only if the emotion has stabilized\n",
    "        if common_emotion != current_emotion and emotion_buffer.count(common_emotion) > 15:\n",
    "            current_emotion = common_emotion\n",
    "            y_shifted, sr = manipulate_audio(neutral_song_path, current_emotion)\n",
    "            sf.write('modified_song.wav', y_shifted, sr)\n",
    "            pygame.mixer.music.fadeout(1000)  # Fade out current music\n",
    "            pygame.mixer.music.load('modified_song.wav')  # Load new song\n",
    "            pygame.mixer.music.play()\n",
    "\n",
    "    # Display the frame with face and emotion overlay\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
