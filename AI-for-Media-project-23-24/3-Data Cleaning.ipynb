{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an Image Classifier \n",
    "The following code has been adapted from various sources including: https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/blob/main/Week-3-CNNs-and-image-classification/train-image-classifier-from-pretrained.ipynbdebugged andareas have been debugged using GPT.\n",
    "In this notebook I clean my image dataset, create new data points for each image and implement these changes back into my 'emtional-class-datasheet.csv'. \n",
    "Additionally, I implement ResNet and VisionTransformer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:50:53.983450Z",
     "iopub.status.busy": "2024-09-05T13:50:53.983157Z",
     "iopub.status.idle": "2024-09-05T13:50:54.673755Z",
     "shell.execute_reply": "2024-09-05T13:50:54.672748Z",
     "shell.execute_reply.started": "2024-09-05T13:50:53.983420Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install opencv-python\n",
    "%pip install torch torchvision\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install pytesseract\n",
    "%pip install scikit-learn\n",
    "%pip install dlib\n",
    "%pip install opencv-python\n",
    "%pip install cmake\n",
    "%pip install --upgrade numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall opencv-python\n",
    "%pip uninstall opencv-python-headless\n",
    "%pip install opencv-python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:50:54.675885Z",
     "iopub.status.busy": "2024-09-05T13:50:54.675628Z",
     "iopub.status.idle": "2024-09-05T13:50:58.134809Z",
     "shell.execute_reply": "2024-09-05T13:50:58.133470Z",
     "shell.execute_reply.started": "2024-09-05T13:50:54.675855Z"
    }
   },
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data Cleaning.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageFolder\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_chunking\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[39m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     17\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/validation.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/_array_api.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n\u001b[1;32m     13\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marray_api_compat.numpy\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/sklearn/utils/fixes.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternals\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_packaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparallel\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_threadpool_controller\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/stats/__init__.py:610\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m \n\u001b[1;32m    606\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    609\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    611\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[1;32m    612\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/stats/_stats_py.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Import unused here but needs to stay until end of deprecation periode\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m# See https://github.com/scipy/scipy/issues/15765#issuecomment-1875564522\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _mstats_basic \u001b[39mas\u001b[39;00m mstats_basic\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_mstats_common\u001b[39;00m \u001b[39mimport\u001b[39;00m _find_repeats, theilslopes, siegelslopes\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/stats/distributions.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Author:  Travis Oliphant  2002-2011 with contributions from\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#          SciPy Developers 2004-2011\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m#       instead of `git blame -Lxxx,+x`.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_distn_infrastructure\u001b[39;00m \u001b[39mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _discrete_distns\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_continuous_distns\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/stats/_continuous_distns.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolynomial\u001b[39;00m \u001b[39mimport\u001b[39;00m Polynomial\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minterpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m BSpline\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdoccer\u001b[39;00m \u001b[39mimport\u001b[39;00m (extend_notes_in_docstring,\n\u001b[1;32m     14\u001b[0m                                replace_notes_in_docstring,\n\u001b[1;32m     15\u001b[0m                                inherit_docstring_from)\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ccallback\u001b[39;00m \u001b[39mimport\u001b[39;00m LowLevelCallable\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/interpolate/__init__.py:167\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m========================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mInterpolation (:mod:`scipy.interpolate`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m(should not be used in new code).\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_interpolate\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fitpack_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m    170\u001b[0m \u001b[39m# New interface to fitpack library:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/interpolate/_interpolate.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_lib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_util\u001b[39;00m \u001b[39mimport\u001b[39;00m copy_if_needed\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mimport\u001b[39;00m comb\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _fitpack_py\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_polyint\u001b[39;00m \u001b[39mimport\u001b[39;00m _Interpolator1D\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _ppoly\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/interpolate/_fitpack_py.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# These are in the API for fitpack even if not used in fitpack.py itself.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_fitpack_impl\u001b[39;00m \u001b[39mimport\u001b[39;00m bisplrep, bisplev, dblint  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _fitpack_impl \u001b[39mas\u001b[39;00m _impl\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bsplines\u001b[39;00m \u001b[39mimport\u001b[39;00m BSpline\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/scipy/interpolate/_fitpack_impl.py:103\u001b[0m\n\u001b[1;32m     52\u001b[0m _iermess \u001b[39m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m     \u001b[39m0\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mThe spline has a residual sum of squares fp such that \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mabs(fp-s)/s<=0.001\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mAn error occurred\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTypeError\u001b[39;00m]\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     71\u001b[0m _iermess2 \u001b[39m=\u001b[39m {\n\u001b[1;32m     72\u001b[0m     \u001b[39m0\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mThe spline has a residual sum of squares fp such that \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mabs(fp-s)/s<=0.001\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[39m'\u001b[39m\u001b[39munknown\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39mAn error occurred\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTypeError\u001b[39;00m]\n\u001b[1;32m    100\u001b[0m }\n\u001b[1;32m    102\u001b[0m _parcur_cache \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m: array([], \u001b[39mfloat\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mwrk\u001b[39m\u001b[39m'\u001b[39m: array([], \u001b[39mfloat\u001b[39m),\n\u001b[0;32m--> 103\u001b[0m                  \u001b[39m'\u001b[39m\u001b[39miwrk\u001b[39m\u001b[39m'\u001b[39m: array([], dfitpack_int), \u001b[39m'\u001b[39m\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m: array([], \u001b[39mfloat\u001b[39m),\n\u001b[1;32m    104\u001b[0m                  \u001b[39m'\u001b[39m\u001b[39mub\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m}\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplprep\u001b[39m(x, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, u\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ub\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ue\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, k\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, task\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, s\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, t\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m             full_output\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, nest\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, per\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, quiet\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    109\u001b[0m     \u001b[39m# see the docstring of `_fitpack_py/splprep`\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m task \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:46\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(dtype):\n\u001b[0;32m---> 46\u001b[0m     arg_str \u001b[39m=\u001b[39m _construction_repr(dtype, include_align\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39misalignedstruct:\n\u001b[1;32m     48\u001b[0m         arg_str \u001b[39m=\u001b[39m arg_str \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, align=True\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:100\u001b[0m, in \u001b[0;36m_construction_repr\u001b[0;34m(dtype, include_align, short)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m _subarray_str(dtype)\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m _scalar_str(dtype, short\u001b[39m=\u001b[39mshort)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:143\u001b[0m, in \u001b[0;36m_scalar_str\u001b[0;34m(dtype, short)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mtimedelta64:\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39mm8\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, _datetime_metadata_str(dtype))\n\u001b[0;32m--> 143\u001b[0m \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(dtype, np\u001b[39m.\u001b[39mnumber):\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Short repr with endianness, like '<f8'\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m short \u001b[39mor\u001b[39;00m dtype\u001b[39m.\u001b[39mbyteorder \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%c\u001b[39;00m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, dtype\u001b[39m.\u001b[39mkind, dtype\u001b[39m.\u001b[39mitemsize)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/numerictypes.py:417\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 417\u001b[0m     arg1 \u001b[39m=\u001b[39m dtype(arg1)\u001b[39m.\u001b[39mtype\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    419\u001b[0m     arg2 \u001b[39m=\u001b[39m dtype(arg2)\u001b[39m.\u001b[39mtype\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:46\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(dtype):\n\u001b[0;32m---> 46\u001b[0m     arg_str \u001b[39m=\u001b[39m _construction_repr(dtype, include_align\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39misalignedstruct:\n\u001b[1;32m     48\u001b[0m         arg_str \u001b[39m=\u001b[39m arg_str \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, align=True\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:100\u001b[0m, in \u001b[0;36m_construction_repr\u001b[0;34m(dtype, include_align, short)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m _subarray_str(dtype)\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m _scalar_str(dtype, short\u001b[39m=\u001b[39mshort)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:143\u001b[0m, in \u001b[0;36m_scalar_str\u001b[0;34m(dtype, short)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mtimedelta64:\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39mm8\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, _datetime_metadata_str(dtype))\n\u001b[0;32m--> 143\u001b[0m \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(dtype, np\u001b[39m.\u001b[39mnumber):\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Short repr with endianness, like '<f8'\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m short \u001b[39mor\u001b[39;00m dtype\u001b[39m.\u001b[39mbyteorder \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%c\u001b[39;00m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, dtype\u001b[39m.\u001b[39mkind, dtype\u001b[39m.\u001b[39mitemsize)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/numerictypes.py:417\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 417\u001b[0m     arg1 \u001b[39m=\u001b[39m dtype(arg1)\u001b[39m.\u001b[39mtype\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    419\u001b[0m     arg2 \u001b[39m=\u001b[39m dtype(arg2)\u001b[39m.\u001b[39mtype\n",
      "    \u001b[0;31m[... skipping similar frames: __repr__ at line 46 (715 times), _construction_repr at line 100 (714 times), _scalar_str at line 143 (714 times), issubdtype at line 417 (714 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:100\u001b[0m, in \u001b[0;36m_construction_repr\u001b[0;34m(dtype, include_align, short)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m _subarray_str(dtype)\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m _scalar_str(dtype, short\u001b[39m=\u001b[39mshort)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:143\u001b[0m, in \u001b[0;36m_scalar_str\u001b[0;34m(dtype, short)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mtimedelta64:\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39mm8\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, _datetime_metadata_str(dtype))\n\u001b[0;32m--> 143\u001b[0m \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(dtype, np\u001b[39m.\u001b[39mnumber):\n\u001b[1;32m    144\u001b[0m     \u001b[39m# Short repr with endianness, like '<f8'\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m short \u001b[39mor\u001b[39;00m dtype\u001b[39m.\u001b[39mbyteorder \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%c\u001b[39;00m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (byteorder, dtype\u001b[39m.\u001b[39mkind, dtype\u001b[39m.\u001b[39mitemsize)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/numerictypes.py:417\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mReturns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg1, generic):\n\u001b[0;32m--> 417\u001b[0m     arg1 \u001b[39m=\u001b[39m dtype(arg1)\u001b[39m.\u001b[39mtype\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issubclass_(arg2, generic):\n\u001b[1;32m    419\u001b[0m     arg2 \u001b[39m=\u001b[39m dtype(arg2)\u001b[39m.\u001b[39mtype\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/numpy/core/_dtype.py:46\u001b[0m, in \u001b[0;36m__repr__\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(dtype):\n\u001b[0;32m---> 46\u001b[0m     arg_str \u001b[39m=\u001b[39m _construction_repr(dtype, include_align\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39misalignedstruct:\n\u001b[1;32m     48\u001b[0m         arg_str \u001b[39m=\u001b[39m arg_str \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, align=True\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "import pytesseract\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:50:58.139105Z",
     "iopub.status.busy": "2024-09-05T13:50:58.137977Z",
     "iopub.status.idle": "2024-09-05T13:50:58.153717Z",
     "shell.execute_reply": "2024-09-05T13:50:58.151988Z",
     "shell.execute_reply.started": "2024-09-05T13:50:58.139075Z"
    }
   },
   "outputs": [],
   "source": [
    "# Double check my class datasets are recognised:\n",
    "os.listdir('/Users/nixi/Desktop/Final thesis folder 24/AI-for-Media-project-23-24/my-classification-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:50:58.156534Z",
     "iopub.status.busy": "2024-09-05T13:50:58.155470Z",
     "iopub.status.idle": "2024-09-05T13:50:58.168830Z",
     "shell.execute_reply": "2024-09-05T13:50:58.167081Z",
     "shell.execute_reply.started": "2024-09-05T13:50:58.156485Z"
    }
   },
   "outputs": [],
   "source": [
    "# I had an unexpected file in my class dataset ('.ipynb_checkpoints') and wanted to double chheck the conentss, if any.\n",
    "directory_path = '/Users/nixi/Desktop/Final thesis folder 24/AI-for-Media-project-23-24/my-classification-dataset'\n",
    "\n",
    "# List the contents of the directory\n",
    "contents = os.listdir(directory_path)\n",
    "\n",
    "if '.ipynb_checkpoints' in contents:\n",
    "    checkpoint_path = os.path.join(directory_path, '.ipynb_checkpoints')\n",
    "    checkpoint_contents = os.listdir(checkpoint_path)\n",
    "    print(checkpoint_contents)\n",
    "else:\n",
    "    print(\".ipynb_checkpoints directory does not exist in the specified directory.\")\n",
    "    # This created an empty list, nothing to worry about!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "### A. Remove any duplicates in 'my-classification-dataset' \n",
    "I had noticed a few duplicates when woorking with the same custom dataset for another data science module for k-means cluster experimentation. The private repository can be found here: https://git.arts.ac.uk/23002156/Intro-Data-Science-Portfolio-2024.\n",
    "### B. Isolating faces from images ( particularly for magazine data and to remove solely text data thta may have been collected during sampling)\n",
    "I could exclude images with text using a text detector, but this may reduce the dataset significantly.\n",
    "Use Haar Cascade classifier ( which uses feature based object detection) available from: https://github.com/opencv/opencv/tree/4.x/data/haarcascades.\n",
    "### C. Create 'emotional-class-datasheet.csv', considering any new changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "../numpy/_core/src/multiarray/iterators.c:192: bad argument to internal function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data Cleaning.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m                 data\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mImage\u001b[39m\u001b[39m'\u001b[39m: filename, \u001b[39m'\u001b[39m\u001b[39mEmotionClass\u001b[39m\u001b[39m'\u001b[39m: emotion_class, \u001b[39m'\u001b[39m\u001b[39mTotalFaceArea\u001b[39m\u001b[39m'\u001b[39m: w \u001b[39m*\u001b[39m h, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mEyesX\u001b[39m\u001b[39m'\u001b[39m: eyes_x, \u001b[39m'\u001b[39m\u001b[39mEyesY\u001b[39m\u001b[39m'\u001b[39m: eyes_y, \u001b[39m'\u001b[39m\u001b[39mNoseX\u001b[39m\u001b[39m'\u001b[39m: nose_x, \u001b[39m'\u001b[39m\u001b[39mNoseY\u001b[39m\u001b[39m'\u001b[39m: nose_y, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m                              \u001b[39m'\u001b[39m\u001b[39mMouthX\u001b[39m\u001b[39m'\u001b[39m: mouth_x, \u001b[39m'\u001b[39m\u001b[39mMouthY\u001b[39m\u001b[39m'\u001b[39m: mouth_y})\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# Create a DataFrame to store the new data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# Write the data to a CSV file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/3-Data%20Cleaning.ipynb#X12sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mfacial_landmarks_data.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/frame.py:876\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    867\u001b[0m             mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[1;32m    868\u001b[0m                 data,\n\u001b[1;32m    869\u001b[0m                 index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 typ\u001b[39m=\u001b[39mmanager,\n\u001b[1;32m    874\u001b[0m             )\n\u001b[1;32m    875\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 876\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    877\u001b[0m             {},\n\u001b[1;32m    878\u001b[0m             index,\n\u001b[1;32m    879\u001b[0m             columns \u001b[39mif\u001b[39;49;00m columns \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m default_index(\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    880\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    881\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[1;32m    882\u001b[0m         )\n\u001b[1;32m    883\u001b[0m \u001b[39m# For data is scalar\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/internals/construction.py:444\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mseries\u001b[39;00m \u001b[39mimport\u001b[39;00m Series\n\u001b[1;32m    443\u001b[0m arrays \u001b[39m=\u001b[39m Series(data, index\u001b[39m=\u001b[39mcolumns, dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m)\n\u001b[0;32m--> 444\u001b[0m missing \u001b[39m=\u001b[39m arrays\u001b[39m.\u001b[39;49misna()\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[39m# GH10856\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[39m# raise ValueError if only scalars in dict\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     index \u001b[39m=\u001b[39m _extract_index(arrays[\u001b[39m~\u001b[39mmissing])\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/series.py:5775\u001b[0m, in \u001b[0;36mSeries.isna\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5773\u001b[0m \u001b[39m@doc\u001b[39m(NDFrame\u001b[39m.\u001b[39misna, klass\u001b[39m=\u001b[39m_shared_doc_kwargs[\u001b[39m\"\u001b[39m\u001b[39mklass\u001b[39m\u001b[39m\"\u001b[39m])  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   5774\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[0;32m-> 5775\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39;49misna(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/generic.py:8754\u001b[0m, in \u001b[0;36mNDFrame.isna\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   8693\u001b[0m \u001b[39m@doc\u001b[39m(klass\u001b[39m=\u001b[39m_shared_doc_kwargs[\u001b[39m\"\u001b[39m\u001b[39mklass\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m   8694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   8695\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   8696\u001b[0m \u001b[39m    Detect missing values.\u001b[39;00m\n\u001b[1;32m   8697\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8752\u001b[0m \u001b[39m    dtype: bool\u001b[39;00m\n\u001b[1;32m   8753\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 8754\u001b[0m     \u001b[39mreturn\u001b[39;00m isna(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39misna\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/dtypes/missing.py:178\u001b[0m, in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(obj: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mbool_] \u001b[39m|\u001b[39m NDFrame:\n\u001b[1;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/dtypes/missing.py:216\u001b[0m, in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna_array(obj\u001b[39m.\u001b[39m_values, inf_as_na\u001b[39m=\u001b[39minf_as_na)\n\u001b[1;32m    215\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, ABCSeries):\n\u001b[0;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m _isna_array(obj\u001b[39m.\u001b[39;49m_values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    217\u001b[0m     \u001b[39m# box\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     result \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_constructor(result, index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex, name\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mname, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/dtypes/missing.py:292\u001b[0m, in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    290\u001b[0m     result \u001b[39m=\u001b[39m _isna_recarray_dtype(values, inf_as_na\u001b[39m=\u001b[39minf_as_na)\n\u001b[1;32m    291\u001b[0m \u001b[39melif\u001b[39;00m is_string_or_object_np_dtype(values\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m--> 292\u001b[0m     result \u001b[39m=\u001b[39m _isna_string_dtype(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39m# this is the NaT pattern\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mview(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m iNaT\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/pandas/core/dtypes/missing.py:313\u001b[0m, in \u001b[0;36m_isna_string_dtype\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39min\u001b[39;00m {\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m--> 313\u001b[0m         result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39;49misnaobj(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         \u001b[39m# 0-D, reached via e.g. mask_missing\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39misnaobj(values\u001b[39m.\u001b[39mravel(), inf_as_na\u001b[39m=\u001b[39minf_as_na)\n",
      "File \u001b[0;32mmissing.pyx:187\u001b[0m, in \u001b[0;36mpandas._libs.missing.isnaobj\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmissing.pyx:212\u001b[0m, in \u001b[0;36mpandas._libs.missing.isnaobj\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: ../numpy/_core/src/multiarray/iterators.c:192: bad argument to internal function"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# The following code has been adapted from Week 4,Sensing bodies : https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/commit/981594f418f30d02fcff4998cc93f39d4108b608 and debugged using GPT\n",
    "# Isolate faces using Haar Casscade Classifier, I learnt how to implement this classifier from: https://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_Image_Object_Detection_Face_Detection_Haar_Cascade_Classifiers.php\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "# Load in facial landmark predictor \n",
    "predictor_path = 'shape_predictor_68_face_landmarks (1).dat'\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# Path to the directory containing your images\n",
    "directory_path = '/Users/nixi/Desktop/Final thesis folder 24/AI-for-Media-project-23-24/my-classification-dataset'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate over all files in the directory, as adapted from: https://www.slingacademy.com/article/python-how-to-iterate-over-all-files-in-a-directory/\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            # Load the image\n",
    "            image_path = os.path.join(root, filename)\n",
    "            print(\"Processing image\", image_path)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Check if image is loaded properly\n",
    "            if image is None:\n",
    "                print(f\"Error loading image {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Convert the image to grayscale for feature extraction, which helps to reduce any noise when the images are processed and hopefully better training.\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the image\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            # If there aren't any faces present, print so.\n",
    "            if len(faces) == 0:\n",
    "                print(\"No faces detected in\", image_path)\n",
    "                # If no faces present then remove the file from my dataset entirely.\n",
    "                data.append({'Image': filename, 'Path': image_path, 'NumFaces': len(faces)})\n",
    "                print(image_path,\"removed from dataset\")\n",
    "                continue\n",
    "\n",
    "            # Extract emotion class from the directory path to add this to my DataFrame.\n",
    "            emotion_class = os.path.basename(root)\n",
    "            \n",
    "            # Next I need to find eye, mouth and nose landmark points for each image. I started with creating variables to store the data.\n",
    "            # Code debugged  using GPT and adapted from https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/tree/main/Week-4-Sensing-bodies and https://dontrepeatyourself.org/post/how-to-detect-face-landmarks-with-dlib-python-and-opencv/?utm_content=cmp-true\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_rect = dlib.rectangle(int(x), int(y), int(x + w), int(y + h))\n",
    "                landmarks = predictor(gray, face_rect)\n",
    "\n",
    "                # Initialize lists for landmarks\n",
    "                eyes_x, eyes_y, nose_x, nose_y, mouth_x, mouth_y = [], [], [], [], [], []\n",
    "\n",
    "                # Extract landmark coordinates for eyes\n",
    "                for n in range(36, 48):\n",
    "                    x = landmarks.part(n).x\n",
    "                    y = landmarks.part(n).y\n",
    "                    eyes_x.append(x)\n",
    "                    eyes_y.append(y)\n",
    "\n",
    "                # Extract landmark coordinates for nose\n",
    "                for n in range(27, 35):\n",
    "                    x = landmarks.part(n).x\n",
    "                    y = landmarks.part(n).y\n",
    "                    nose_x.append(x)\n",
    "                    nose_y.append(y)\n",
    "\n",
    "                # Extract landmark coordinates for mouth\n",
    "                for n in range(48, 68):\n",
    "                    x = landmarks.part(n).x\n",
    "                    y = landmarks.part(n).y\n",
    "                    mouth_x.append(x)\n",
    "                    mouth_y.append(y)\n",
    "\n",
    "                # Append extracted features to list\n",
    "                data.append({'Image': filename, 'EmotionClass': emotion_class, 'TotalFaceArea': w * h, \n",
    "                             'EyesX': eyes_x, 'EyesY': eyes_y, 'NoseX': nose_x, 'NoseY': nose_y, \n",
    "                             'MouthX': mouth_x, 'MouthY': mouth_y})\n",
    "\n",
    "# Create a DataFrame to store the new data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write the data to a CSV file\n",
    "df.to_csv('facial_landmarks_data.csv', index=False)\n",
    "\n",
    "# To signify that all processes have been completed\n",
    "print(\"Data processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T13:50:58.172266Z",
     "iopub.status.busy": "2024-09-05T13:50:58.171029Z",
     "iopub.status.idle": "2024-09-05T13:51:27.287909Z",
     "shell.execute_reply": "2024-09-05T13:51:27.287004Z",
     "shell.execute_reply.started": "2024-09-05T13:50:58.172214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code adapted from: https://www.geeksforgeeks.org/reading-image-opencv-using-python/?ref=lbp \n",
    "# Remove Duplicates:\n",
    "\n",
    "import hashlib\n",
    "\n",
    "folder_path = '/Users/nixi/Desktop/Final thesis folder 24/AI-for-Media-project-23-24/my-classification-dataset'\n",
    "\n",
    "# Load CSV file\n",
    "csv_file_path = 'emotional-class-datasheet.csv'\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Initialize a dictionary to store file hashes\n",
    "file_hashes = {}\n",
    "\n",
    "# Iterate through the subfolders\n",
    "for subdir, dirs, files in os.walk(folder_path):\n",
    "    # Iterate through the files in each subfolder\n",
    "    for filename in files:\n",
    "        file_path = os.path.join(subdir, filename)\n",
    "        \n",
    "        # Calculate the hash of the file content\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_hash = hashlib.md5(file.read()).hexdigest()\n",
    "        \n",
    "        # Check for duplicates based on file content\n",
    "        if file_hash in file_hashes.values():\n",
    "            # Remove duplicate file\n",
    "            os.remove(file_path)\n",
    "            # Remove corresponding row from CSV\n",
    "            data = data[data['Image URL'] != file_path]\n",
    "        else:\n",
    "            file_hashes[filename] = file_hash\n",
    "\n",
    "# Save the updated data to the CSV file\n",
    "data.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T14:15:30.961574Z",
     "iopub.status.busy": "2024-09-05T14:15:30.960991Z",
     "iopub.status.idle": "2024-09-05T14:15:31.035033Z",
     "shell.execute_reply": "2024-09-05T14:15:31.033899Z",
     "shell.execute_reply.started": "2024-09-05T14:15:30.961546Z"
    }
   },
   "outputs": [],
   "source": [
    "# How has this impacted my ratio?\n",
    "emotional_classes = ['neutral-face', 'happy-face', 'sad-face', 'angry-face']\n",
    "\n",
    "dirrectory = \"/notebooks/AI-for-Media-project-23-24/my-classification-dataset\"\n",
    "\n",
    "#Instantiate for counts\n",
    "class_counts = {}\n",
    "\n",
    "for emotion in emotional_classes:\n",
    "    class_dir = os.path.join(dirrectory, emotion)\n",
    "    # Count the number of image files in the class directory\n",
    "    class_counts[emotion] = len([file for file in os.listdir(class_dir) if file.endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp'))])\n",
    "\n",
    "\n",
    "# Print out the value of each folder\n",
    "for emotion_class, count in class_counts.items():\n",
    "    print(f\"Emotional class: {emotion_class}, Total images: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T14:25:57.460351Z",
     "iopub.status.busy": "2024-09-05T14:25:57.459654Z",
     "iopub.status.idle": "2024-09-05T14:25:57.521001Z",
     "shell.execute_reply": "2024-09-05T14:25:57.520130Z",
     "shell.execute_reply.started": "2024-09-05T14:25:57.460324Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=['EmotionClass'])\n",
    "\n",
    "#df = df.drop(columns=['Path', 'NumFaces'])\n",
    "\n",
    "# Step 2: Inspect NaN values\n",
    "print(\"NaN values in each column before cleaning:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Step 3: Drop rows with NaN values in the 'EmotionClass' column\n",
    "df_cleaned = df.dropna(subset=['EmotionClass'])\n",
    "\n",
    "# Step 4: Fill remaining NaN values in specified columns\n",
    "df_cleaned = df_cleaned.fillna({\n",
    "    'EyesX': '',\n",
    "    'EyesY': '',\n",
    "    'NoseX': '',\n",
    "    'NoseY': '',\n",
    "    'MouthX': '',\n",
    "    'MouthY': ''})\n",
    "\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T14:26:07.070535Z",
     "iopub.status.busy": "2024-09-05T14:26:07.069995Z",
     "iopub.status.idle": "2024-09-05T14:26:07.171127Z",
     "shell.execute_reply": "2024-09-05T14:26:07.169732Z",
     "shell.execute_reply.started": "2024-09-05T14:26:07.070443Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Update 'cleaned_dataset.csv'\n",
    "df_cleaned.to_csv('final_cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T14:26:08.209858Z",
     "iopub.status.busy": "2024-09-05T14:26:08.209482Z",
     "iopub.status.idle": "2024-09-05T14:26:15.363328Z",
     "shell.execute_reply": "2024-09-05T14:26:15.360571Z",
     "shell.execute_reply.started": "2024-09-05T14:26:08.209829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce the sample to remove bias in model training to predict emotions.\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "emotional_classes = ['neutral-face', 'happy-face', 'sad-face', 'angry-face']\n",
    "\n",
    "# Directory containing the images\n",
    "directory = \"/notebooks/AI-for-Media-project-23-24/my-classification-dataset\"\n",
    "\n",
    "# Output directory for reduced image sets\n",
    "output_dir = \"/notebooks/AI-for-Media-project-23-24/reduced-dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Count and reduce images\n",
    "for emotion in emotional_classes:\n",
    "    class_dir = os.path.join(directory, emotion)\n",
    "    images = [file for file in os.listdir(class_dir) if file.endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp'))]\n",
    "    \n",
    "    # Randomly select up to 1500 images\n",
    "    selected_images = random.sample(images, min(len(images), 1500))\n",
    "    \n",
    "    # Create output directory for the emotion class\n",
    "    class_output_dir = os.path.join(output_dir, emotion)\n",
    "    os.makedirs(class_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy selected images to the output directory\n",
    "    for image in selected_images:\n",
    "        shutil.copy(os.path.join(class_dir, image), os.path.join(class_output_dir, image))\n",
    "    \n",
    "    # Print out the value of each folder\n",
    "    print(f\"Emotional class: {emotion}, Total images after reduction: {len(selected_images)}\")\n",
    "\n",
    "print(\"Image selection and saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the training and validation set for training\n",
    "I experimented with how to best split my dataset into train and validation sets, while maintaining the directory structure. I tried manually doing so, then using 'random_state= 42' as used in the class workbook which split data in a deterministic way. (See https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/tree/main/Week-3-CNNs-and-image-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T14:16:19.319016Z",
     "iopub.status.busy": "2024-09-05T14:16:19.318012Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "dataset_path = '/notebooks/AI-for-Media-project-23-24/reduced-dataset'\n",
    "output_path = '/notebooks/AI-for-Media-project-23-24/my-classification-split'\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Create output directories for train and validation sets\n",
    "train_output_path = os.path.join(output_path, 'train')\n",
    "val_output_path = os.path.join(output_path, 'validation')\n",
    "os.makedirs(train_output_path, exist_ok=True)\n",
    "os.makedirs(val_output_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to hold file paths by emotion class\n",
    "class_files = {}\n",
    "\n",
    "# Collect image and label files by emotion class\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    for f in files:\n",
    "        if f.endswith('.jpg') or f.endswith('.png') or f.endswith('.txt'):\n",
    "            class_name = os.path.basename(root)\n",
    "            if class_name not in class_files:\n",
    "                class_files[class_name] = {'images': [], 'labels': []}\n",
    "            file_path = os.path.join(root, f)\n",
    "            if f.endswith('.jpg') or f.endswith('.png'):\n",
    "                class_files[class_name]['images'].append(file_path)\n",
    "            elif f.endswith('.txt'):\n",
    "                class_files[class_name]['labels'].append(file_path)\n",
    "\n",
    "# Perform train-test split for each class separately and copy the files\n",
    "for class_name, files in class_files.items():\n",
    "    train_image_files, val_image_files = train_test_split(files['images'], test_size=1 - train_ratio, random_state=42)\n",
    "    train_label_files = [label_file for label_file in files['labels'] if os.path.splitext(label_file)[0] in train_image_files]\n",
    "    val_label_files = [label_file for label_file in files['labels'] if os.path.splitext(label_file)[0] in val_image_files]\n",
    "\n",
    "    # Copy training image and label files to the appropriate directory\n",
    "    for train_image_file in train_image_files:\n",
    "        relative_dir = os.path.relpath(os.path.dirname(train_image_file), dataset_path)\n",
    "        target_dir = os.path.join(train_output_path, relative_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        shutil.copy(train_image_file, os.path.join(target_dir, os.path.basename(train_image_file)))\n",
    "\n",
    "    for train_label_file in train_label_files:\n",
    "        relative_dir = os.path.relpath(os.path.dirname(train_label_file), dataset_path)\n",
    "        target_dir = os.path.join(train_output_path, relative_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        shutil.copy(train_label_file, os.path.join(target_dir, os.path.basename(train_label_file)))\n",
    "\n",
    "    # Copy validation image and label files to the appropriate directory\n",
    "    for val_image_file in val_image_files:\n",
    "        relative_dir = os.path.relpath(os.path.dirname(val_image_file), dataset_path)\n",
    "        target_dir = os.path.join(val_output_path, relative_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        shutil.copy(val_image_file, os.path.join(target_dir, os.path.basename(val_image_file)))\n",
    "\n",
    "    for val_label_file in val_label_files:\n",
    "        relative_dir = os.path.relpath(os.path.dirname(val_label_file), dataset_path)\n",
    "        target_dir = os.path.join(val_output_path, relative_dir)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        shutil.copy(val_label_file, os.path.join(target_dir, os.path.basename(val_label_file)))\n",
    "\n",
    "print(\"Data split into train and validation sets for each class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had a folder called 'ipynb_checkpoints' that was considered a 7th emotional class but had no data within. This prevented the above code from working because file formats listed were not found at all.\n",
    "# This seemed to be a reoccuring issue for image classification processes for other python users. This was found out at: https://forums.fast.ai/t/how-to-remove-ipynb-checkpoint/8532/27.\n",
    "# I resorted to deleting the file overall and updating the training ( below code) from  7 classes to 6 and re-running training. ( code adapted from: https://stackoverflow.com/questions/61207135/how-can-i-ignore-or-remove-ipynb-checkpoints-in-colab)\n",
    "dataset_dir = '/notebooks/AI-for-Media-project-23-24/my-classification-split/train' \n",
    "\n",
    "# Remove the .ipynb_checkpoints directory if it exists\n",
    "ipynb_checkpoints_dir = os.path.join(dataset_dir, '.ipynb_checkpoints')\n",
    "if os.path.exists(ipynb_checkpoints_dir):\n",
    "    os.rmdir(ipynb_checkpoints_dir)\n",
    "    print(\".ipynb_checkpoints directory deleted successfully.\")\n",
    "else:\n",
    "    print(\".ipynb_checkpoints directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from https://www.kaggle.com/code/akshitmadan/emotion-classification-cnn-using-keras.\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "picture_size = 150\n",
    "\n",
    "folder_path = '/notebooks/AI-for-Media-project-23-24/my-classification-split/'\n",
    "\n",
    "no_of_classes = 4 # total number of subfolders in my dataset\n",
    "\n",
    "# Generate data\n",
    "batch_size = 300\n",
    "datagen_train = ImageDataGenerator()\n",
    "datagen_val = ImageDataGenerator()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(folder_path + \"train\",\n",
    "                                              target_size=(picture_size, picture_size),\n",
    "                                              color_mode=\"grayscale\",\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "\n",
    "test_set = datagen_val.flow_from_directory(folder_path + \"validation\",\n",
    "                                           target_size=(picture_size, picture_size),\n",
    "                                           color_mode=\"grayscale\",\n",
    "                                           batch_size=batch_size,\n",
    "                                           class_mode='categorical',\n",
    "                                           shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train validation and training image transforms\n",
    "This includes data augmentation using PyTorch and ImageDataGenerator from Kera, which are useful in creating variations in the image data. The following code has also been adapted from class week 3 CNN notebook as stated above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.443395Z",
     "iopub.status.idle": "2024-09-05T12:22:31.443647Z",
     "shell.execute_reply": "2024-09-05T12:22:31.443547Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.443536Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.444207Z",
     "iopub.status.idle": "2024-09-05T12:22:31.444505Z",
     "shell.execute_reply": "2024-09-05T12:22:31.444387Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.444377Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Define train_transform for data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=(-30, 30), translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Define the directory to your training dataset\n",
    "train_path = '/notebooks/AI-for-Media-project-23-24/my-classification-split/train'\n",
    "\n",
    "# Load the training dataset\n",
    "train_set = torchvision.datasets.ImageFolder(root=train_path, transform=train_transform)\n",
    "\n",
    "# Define DataLoader for batch loading\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get a batch of training images\n",
    "real_batch = next(iter(train_loader))\n",
    "tensor_batch = real_batch[0]\n",
    "\n",
    "# Convert a batch of PyTorch tensors to NumPy arrays\n",
    "train_numpy = tensor_batch.numpy()\n",
    "train_numpy = np.moveaxis(train_numpy, 1, -1)\n",
    "print(\"Shape of train_numpy:\", train_numpy.shape)\n",
    "\n",
    "# Plot some training images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "\n",
    "# Display the grid of images\n",
    "plt.imshow(make_grid(tensor_batch, padding=2, normalize=True).permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.445503Z",
     "iopub.status.idle": "2024-09-05T12:22:31.445687Z",
     "shell.execute_reply": "2024-09-05T12:22:31.445608Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.445599Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define train_transform for data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=(-30,30), translate=(0.15,0.15), scale=(0.85,1.15)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5), # added transformations after the 3rd model run through, which found the training to be 100% beforehand. Code adapted from: https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html?highlight=data+augmentation\n",
    "    transforms.ToTensor(),\n",
    "     transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.Normalize((0.5,), (0.5,)) \n",
    "])\n",
    "\n",
    "# Define the directory to your training dataset\n",
    "train_path = '/notebooks/AI-for-Media-project-23-24/my-classification-split/train'  # Path to the directory containing class directories\n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder(root=train_path,\n",
    "                                              transform=train_transform)\n",
    "\n",
    "# Convert PyTorch tensors to NumPy arrays (had to convert PyTorch tensor to NumPy array, due to incorrect data processing.\n",
    "# I used code adapted from: https://www.geeksforgeeks.org/how-to-convert-pytorch-tensor-to-numpy-array/ then debugged with GPT after my own attempt.\n",
    "train_numpy = np.array([x.numpy() for x, _ in train_set])\n",
    "\n",
    "train_numpy = np.moveaxis(train_numpy, 1, -1)\n",
    "print(\"Shape of train_numpy:\", train_numpy.shape)\n",
    "\n",
    "# Define DataLoader for batch loading\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get a batch of training images\n",
    "real_batch = next(iter(train_loader))\n",
    "\n",
    "tensor_batch = real_batch[0]\n",
    "\n",
    "# Plot some training images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "\n",
    "# The below code was adapted from https://stackoverflow.com/questions/51329159/how-can-i-generate-and-display-a-grid-of-images-in-pytorch-with-plt-imshow-and-t, which explained the same issue where matplotlib was not displaying my images because of image sizing differences.\n",
    "plt.imshow(make_grid(tensor_batch, padding=2, normalize=True).permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.446155Z",
     "iopub.status.idle": "2024-09-05T12:22:31.446333Z",
     "shell.execute_reply": "2024-09-05T12:22:31.446255Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.446246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation image transforms using the similar approach as train\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    torchvision.transforms.Resize((150, 150)), \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Define the directory to your validation dataset\n",
    "val_path = '/notebooks/AI-for-Media-project-23-24/my-classification-split/validation'  # Path to the directory containing class directories\n",
    "\n",
    "val_set = torchvision.datasets.ImageFolder(root=val_path, transform=val_transform)\n",
    "\n",
    "# Convert to NumPy array for data processing ( same as the above training data)\n",
    "# I had an issue with the dimensions of my images\n",
    "val_numpy = np.array([x.numpy() for x, _ in val_set])\n",
    "\n",
    "# Code adapted from: https://stackoverflow.com/questions/57438392/rearranging-axes-in-numpy after an error with the shape of val_numpy (374, 1, 150, 150).\n",
    "    # The error: 'sequential_9\". I have an error 'ValueError: Input 0 of layer \"sequential_9\" is incompatible with the layer: expected shape=(None, 150, 150, 1), found shape=(None, 1, 150, 150)'\n",
    "val_numpy = np.moveaxis(val_numpy, 1, -1)\n",
    "\n",
    "# Print shape \n",
    "print(\"Shape of val_numpy:\", val_numpy.shape)\n",
    "\n",
    "# Define DataLoader for batch loading\n",
    "batch_size = 64\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Get a batch of validation images\n",
    "real_batch = next(iter(val_loader))\n",
    "\n",
    "# Convert torch tensor to numpy array and permute the dimensions to match (batch_size, height, width, channels)\n",
    "tensor_batch = real_batch[0]\n",
    "\n",
    "# Plot some validation images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Validation Images\")\n",
    "\n",
    "# The code below displays the batch of images using matplotlib\n",
    "plt.imshow(make_grid(tensor_batch, padding=2, normalize=True).permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.447225Z",
     "iopub.status.idle": "2024-09-05T12:22:31.447424Z",
     "shell.execute_reply": "2024-09-05T12:22:31.447343Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.447333Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class PrintLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        return inputs\n",
    "\n",
    "# regularization applied to model to prevent overfitting ( of which the training model had 100% accuracy, validation 30%)\n",
    "# Code adapted from: https://medium.com/intelligentmachines/convolutional-neural-network-and-regularization-techniques-with-tensorflow-and-keras-5a09e6e65dc7\n",
    "no_of_classes = 4\n",
    "\n",
    "# Define input shape\n",
    "picture_size = 150\n",
    "input_shape = (picture_size, picture_size, 1)\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "# Added in L1 regularization to improve overfit. Code adapted from: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-keras.md.\n",
    "model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "output_shape = model.layers[-1].output_shape\n",
    "print(\"1st CNN layer shape:\", output_shape[1:])\n",
    "\n",
    "# 1st CNN layer with L2 regularization added as suggested by: https://www.kaggle.com/discussions/questions-and-answers/224485#1231774. \n",
    "model.add(Conv2D(32, (5, 5), padding='same', input_shape=input_shape, kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 2nd CNN layer + L2 regularization\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# I added two more convolutional layers,from the original model, then adjusted the 3rd CNN from 512 down to 256 for to ensure consistent structure.  \n",
    "#l2_reg_strength = 0.0001\n",
    "\n",
    "# 3rd CNN layer\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 4th CNN layer with regularization \n",
    "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 5th CNN layer\n",
    "#model.add(Conv2D(1024, (3, 3), padding='same'))  \n",
    "#print(\"5th CNN layer shape:\", model.output_shape)\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#print(\"After 5th MaxPooling2D layer shape:\", model.output_shape)\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "# 6th CNN layer\n",
    "#model.add(Conv2D(2048, (3, 3), padding='same'))  \n",
    "#print(\"6th CNN layer shape:\", model.output_shape)\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#print(\"After 6th MaxPooling2D layer shape:\", model.output_shape)\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "print(\"After Flatten layer shape:\")\n",
    "model.add(PrintLayer())\n",
    "\n",
    "# Add Dense layer with 256 units and relu activation function, and L2 regularization for kernel and bias\n",
    "#model.add(Dense(256))\n",
    "#print(\"Fully connected 1st layer shape:\", model.output_shape)\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256, kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected 2nd layer\n",
    "model.add(Dense(512, kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(no_of_classes, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "model.add(PrintLayer())\n",
    "# Compile model\n",
    "opt = Adam(learning_rate=0.0001) # increased after accuracy rate was stuck between 20-30%\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 100 # Adjust based on your needs\n",
    "\n",
    "# Define callbacks\n",
    "#checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#callbacks_list = [checkpoint]\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.448272Z",
     "iopub.status.idle": "2024-09-05T12:22:31.448474Z",
     "shell.execute_reply": "2024-09-05T12:22:31.448391Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.448381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model with training and validation data, this trains the model and monitors the validation accuracy, saving the best model wieghts based on validation performance.\n",
    "# Code debugged with GPT\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Clear any previous session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# I ran into a 'ValueError: Failed to find data adapter that can handle input: <class 'torchvision.datasets.folder.ImageFolder'>, <class 'NoneType'>' which suggested my data for the training and validation were not in an expected format before passing them into the model.fit() function\n",
    "# The next two lines of code were suggested by GPT, converting PyTorch dataset to MumPy arrays, which then can be used with Keras.\n",
    "#train_numpy = np.array(train_set)\n",
    "\n",
    "# Convert DataLoader to NumPy arrays for training and validation sets\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size) ### Define val_set\n",
    "\n",
    "train_numpy = []\n",
    "train_labels = []\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    data = np.transpose(data.numpy(), (0, 2, 3, 1))  # Transpose to (batch_size, height, width, channels)\n",
    "    train_numpy.append(data)\n",
    "    train_labels.append(labels.numpy())\n",
    "print(\"Train set converted!\")\n",
    "\n",
    "train_numpy = np.concatenate(train_numpy, axis=0)\n",
    "train_labels = np.concatenate(train_labels, axis=0)\n",
    "\n",
    "val_numpy = []\n",
    "val_labels = []\n",
    "\n",
    "for data, labels in val_loader:\n",
    "    data = np.transpose(data.numpy(), (0, 2, 3, 1))  # Transpose to (batch_size, height, width, channels)\n",
    "    val_numpy.append(data)\n",
    "    val_labels.append(labels.numpy())\n",
    "print(\"Validation set converted!\")\n",
    "\n",
    "val_numpy = np.concatenate(val_numpy, axis=0)\n",
    "val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "# Removing bias within model, as learnt from: https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/.\n",
    "# Class weight calculations. I used the vectors assigned to each emotional class and implemennted a weight for each.\n",
    "# The model seemed to have an affinity for predicting '2' (happy face) and '4' (sad face), suggesting bias. For these classes, I applied smaller weighting values.\n",
    "class_weights = {0: 1.0, 1: 1.0, 2: 0.5, 3: 1.0}\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Encode training labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n",
    "\n",
    "# Convert labels to categorical formatting for 4 classes\n",
    "train_labels_encoded = to_categorical(train_labels_encoded, num_classes=4)\n",
    "print(train_labels_encoded.shape)\n",
    "\n",
    "val_labels_encoded = to_categorical(val_labels_encoded, num_classes=4)\n",
    "print(val_labels_encoded.shape)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\"model_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Reinitialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st CNN layer with larger kernel size and L2 regularization\n",
    "model.add(Conv2D(32, (5, 5), padding='same', input_shape=(150, 150, 1), kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))  # Increased dropout rate\n",
    "\n",
    "# 2nd CNN layer\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))  # Increased dropout rate\n",
    "\n",
    "# 3rd CNN layer\n",
    "model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))  # Increased dropout rate\n",
    "\n",
    "# 4th CNN layer\n",
    "model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))  # Increased dropout rate\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected 1st layer\n",
    "model.add(Dense(256, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Fully connected 2nd layer\n",
    "model.add(Dense(512, kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(4, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile model\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Fit model with training and validation data\n",
    "history = model.fit(\n",
    "    x=train_numpy,\n",
    "    y=train_labels_encoded,\n",
    "    epochs=100,  # Assuming `epochs` is defined\n",
    "    validation_data=(val_numpy, val_labels_encoded),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.449206Z",
     "iopub.status.idle": "2024-09-05T12:22:31.449381Z",
     "shell.execute_reply": "2024-09-05T12:22:31.449303Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.449295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Are the train and val shapes as expected?\n",
    "# code adapted from: https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html\n",
    "print(\"Shape of train_numpy:\", train_numpy.shape)\n",
    "print(\"Shape of train_labels_encoded:\", train_labels_encoded.shape)\n",
    "print(\"Shape of val_numpy:\", val_numpy.shape)\n",
    "print(\"Shape of val_labels_encoded:\", val_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.450195Z",
     "iopub.status.idle": "2024-09-05T12:22:31.450369Z",
     "shell.execute_reply": "2024-09-05T12:22:31.450292Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.450283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the best fit fit model\n",
    "model.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T12:22:31.451162Z",
     "iopub.status.idle": "2024-09-05T12:22:31.451363Z",
     "shell.execute_reply": "2024-09-05T12:22:31.451265Z",
     "shell.execute_reply.started": "2024-09-05T12:22:31.451256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.941016Z",
     "iopub.status.idle": "2024-09-04T21:57:48.941385Z",
     "shell.execute_reply": "2024-09-04T21:57:48.941244Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.941226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.ylabel('Loss', fontsize=11)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.ylabel('Accuracy', fontsize=11)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.943515Z",
     "iopub.status.idle": "2024-09-04T21:57:48.943920Z",
     "shell.execute_reply": "2024-09-04T21:57:48.943754Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.943731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot pecentage accuracy and loss for training and validation sets\n",
    "# I asked GPT to fix the following two lines of code which retrievs the results of the val-train results. I did not know that [-1] was neeeded to fetch the last epoch results.\n",
    "final_training_accuracy = history.history['accuracy'][-1]\n",
    "final_validation_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "# Convert result into %\n",
    "training_accuracy_percentage = final_training_accuracy * 100\n",
    "validation_accuracy_percentage = final_validation_accuracy * 100\n",
    "\n",
    "#Code adapted from: https://www.askpython.com/python/examples/print-a-percentage-value-in-python\n",
    "print(\"Final Training Accuracy: {:.2f}%\".format(training_accuracy_percentage))\n",
    "print(\"Final Validation Accuracy: {:.2f}%\".format(validation_accuracy_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.945813Z",
     "iopub.status.idle": "2024-09-04T21:57:48.946288Z",
     "shell.execute_reply": "2024-09-04T21:57:48.946098Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.946073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "# Code adapted from: https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/blob/main/Week-4-Sensing-bodies/02-train-keypoints-classifier.ipynb.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Make Predictions\n",
    "predictions = model.predict(val_numpy)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 2: Convert Predictions and True Labels\n",
    "true_labels = np.argmax(val_labels_encoded, axis=1)\n",
    "\n",
    "# Step 3: Calculate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Step 4: Plot the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are my emotional classes?\n",
    "Finding out which emotional class labels correspond to which emotion in my directory ( stored as one-hot) as made by ImageFolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.948599Z",
     "iopub.status.idle": "2024-09-04T21:57:48.948986Z",
     "shell.execute_reply": "2024-09-04T21:57:48.948831Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.948812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code adapted using https://discuss.pytorch.org/t/how-to-get-the-class-names-to-class-label-mapping/470 and GPT for debugging.\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Print class info. Code adapted from: https://stackoverflow.com/questions/54734286/how-to-get-class-to-idx-map-for-custom-dataset-in-pytorch\n",
    "for class_name, class_index in sorted(train_set.class_to_idx.items()):\n",
    "    print(class_name,class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model evaluation\n",
    "Code adapted from: https://www.tutorialspoint.com/deep_learning_with_keras/deep_learning_with_keras_evaluating_model_performance.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.950429Z",
     "iopub.status.idle": "2024-09-04T21:57:48.950844Z",
     "shell.execute_reply": "2024-09-04T21:57:48.950675Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.950653Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision,Recall,BinaryAccuracy\n",
    "loss, accuracy = model.evaluate(test_set)\n",
    "\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.953338Z",
     "iopub.status.idle": "2024-09-04T21:57:48.953735Z",
     "shell.execute_reply": "2024-09-04T21:57:48.953590Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.953563Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# Dowload a new image for classification and test the model to see the classification progress. \n",
    "# Code adapted from: https://github.com/MahdiNavaei/Emotion-Classification-with-CNN/blob/main/projrct.ipynb.\n",
    "# I chose a happy face because the model seemed to always classify this emotion best and thus should be the quickest way to identify if the model works overall.\n",
    "\n",
    "img=cv2.imread('new-image.png')\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# resize and add in grayscale to match the train and validation images\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "resize = tf.image.rgb_to_grayscale(resize)\n",
    "resize = tf.expand_dims(resize, axis=0)  # Add batch dimension\n",
    "resize = resize / 255.0  # Normalize the image\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(resize.numpy().squeeze(), cmap='gray')\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.show()\n",
    "\n",
    "#Load in model and image details to see which emotional class is predicted.\n",
    "# Code also adapted from: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "predictions = model.predict(resize)\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "# available class labels\n",
    "class_labels = ['angry-face', 'happy-face', 'neutral-face', 'sad-face']\n",
    "\n",
    "# Print predicted class label\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "print(\"Predicted Class Label:\", predicted_class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.955125Z",
     "iopub.status.idle": "2024-09-04T21:57:48.955546Z",
     "shell.execute_reply": "2024-09-04T21:57:48.955388Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.955368Z"
    }
   },
   "outputs": [],
   "source": [
    "# What about other emotional classes e.g., anger\n",
    "img=cv2.imread('new_image2.png')\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "resize = tf.image.rgb_to_grayscale(resize)\n",
    "resize = tf.expand_dims(resize, axis=0)  # Add batch dimension\n",
    "resize = resize / 255.0  # Normalize the image\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(resize.numpy().squeeze(), cmap='gray')\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.show()\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "predictions = model.predict(resize)\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "# available class labels\n",
    "class_labels = ['angry-face', 'happy-face', 'neutral-face', 'sad-face']\n",
    "\n",
    "# Print predicted class label\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "print(\"Predicted Class Label:\", predicted_class_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.957477Z",
     "iopub.status.idle": "2024-09-04T21:57:48.957834Z",
     "shell.execute_reply": "2024-09-04T21:57:48.957696Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.957679Z"
    }
   },
   "outputs": [],
   "source": [
    "# What about other emotional classes e.g., neutral\n",
    "img=cv2.imread('new_image3.png')\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "resize = tf.image.rgb_to_grayscale(resize)\n",
    "resize = tf.expand_dims(resize, axis=0)  # Add batch dimension\n",
    "resize = resize / 255.0  # Normalize the image\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(resize.numpy().squeeze(), cmap='gray')\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.show()\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "predictions = model.predict(resize)\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "# available class labels\n",
    "class_labels = ['angry-face', 'happy-face', 'neutral-face', 'sad-face']\n",
    "\n",
    "# Print predicted class label\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "print(\"Predicted Class Label:\", predicted_class_label)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.960084Z",
     "iopub.status.idle": "2024-09-04T21:57:48.960650Z",
     "shell.execute_reply": "2024-09-04T21:57:48.960410Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.960381Z"
    }
   },
   "outputs": [],
   "source": [
    "# What about other emotional classes e.g., sad\n",
    "img=cv2.imread('new_image4.png')\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "resize = tf.image.resize(img, (150, 150))\n",
    "resize = tf.image.rgb_to_grayscale(resize)\n",
    "resize = tf.expand_dims(resize, axis=0)  # Add batch dimension\n",
    "resize = resize / 255.0  # Normalize the image\n",
    "\n",
    "# Display the preprocessed image\n",
    "plt.imshow(resize.numpy().squeeze(), cmap='gray')\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.show()\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "predictions = model.predict(resize)\n",
    "predicted_class_index = tf.argmax(predictions, axis=1).numpy()[0]\n",
    "\n",
    "# available class labels\n",
    "class_labels = ['angry-face', 'happy-face', 'neutral-face', 'sad-face']\n",
    "\n",
    "# Print predicted class label\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "print(\"Predicted Class Label:\", predicted_class_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Model to Webcam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.963558Z",
     "iopub.status.idle": "2024-09-04T21:57:48.964570Z",
     "shell.execute_reply": "2024-09-04T21:57:48.964293Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.964258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Can the classifier predict real-time expressions\n",
    "%pip install opencv-python\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.967026Z",
     "iopub.status.idle": "2024-09-04T21:57:48.967609Z",
     "shell.execute_reply": "2024-09-04T21:57:48.967367Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.967338Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Define the labels (adjust according to your classes)\n",
    "labels = ['angry-face', 'happy-face', 'neutral-face', 'sad-face']\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized_frame = cv2.resize(gray, (150, 150))  # Resize to match input shape of the model\n",
    "    img_array = img_to_array(resized_frame)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand dimensions to match model's input shape\n",
    "    img_array = img_array / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    label_index = np.argmax(prediction)\n",
    "    label = labels[label_index]\n",
    "\n",
    "    # Display the resulting frame with prediction\n",
    "    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-04T21:57:48.971826Z",
     "iopub.status.idle": "2024-09-04T21:57:48.972359Z",
     "shell.execute_reply": "2024-09-04T21:57:48.972150Z",
     "shell.execute_reply.started": "2024-09-04T21:57:48.972122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the model:\n",
    "# How effective is the pre-trained model in predicting real-time emotions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
