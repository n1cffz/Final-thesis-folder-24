{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7801ce2",
   "metadata": {},
   "source": [
    "### Pair the Emoton Prediction Model with Musical Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2781cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and file handling\n",
    "import os\n",
    "\n",
    "# Numerical and Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Deep Learning Libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Audio Processing Libraries\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "\n",
    "# Computer Vision Libraries\n",
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Real-time Multimedia Handling Libraries\n",
    "import pygame\n",
    "\n",
    "# Collections for Data Structures\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import Audio as ipd\n",
    "from IPython.display import Audio\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "def show_audio(y, sr, normalise=True):\n",
    "    # Plot the waveform\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the audio player\n",
    "    return Audio(y, rate=sr, normalize=normalise, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate song choice\n",
    "# Initialize pygame mixer\n",
    "pygame.mixer.init()\n",
    "\n",
    "# Load pre-trained models\n",
    "rf_model = joblib.load('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/random_forest_model.pkl')\n",
    "scaler = joblib.load('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/scaler.pkl')\n",
    "cnn_model = load_model('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/model.h5')\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/haar/haarcascade_frontalface_alt2.xml')\n",
    "if face_cascade.empty():\n",
    "    raise IOError(\"Failed to load Haar Cascade file. Please check the file path.\")\n",
    "\n",
    "# Load Dlib's shape predictor for facial landmarks\n",
    "shape_predictor = dlib.shape_predictor('/Users/nixi/Desktop/Final-thesis-folder-24/AI-for-Media-project-23-24/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Define emotions map\n",
    "emotion_map = {0: 'angry', 1: 'happy', 2: 'sad', 3: 'neutral'}\n",
    "\n",
    "# Emotion buffer for smoothing predictions\n",
    "emotion_buffer = deque(maxlen=20)  # Keep last 20 frames\n",
    "\n",
    "# Path to neutral song\n",
    "neutral_song_path = 'Music/neutral/aklim_hep_sende.mp3'\n",
    "\n",
    "# Load and play the initial song\n",
    "pygame.mixer.music.load(neutral_song_path)\n",
    "pygame.mixer.music.play()\n",
    "\n",
    "# Change the song based on the emotion using code adapted from: https://librosa.org/doc/main/generated/librosa.effects.time_stretch.html \n",
    "def change_music_based_on_emotion(current_emotion):\n",
    "    y, sr = librosa.load(neutral_song_path)\n",
    "    if current_emotion == 'happy':\n",
    "        y = librosa.effects.time_stretch(y, rate=1.2)\n",
    "    elif current_emotion == 'sad':\n",
    "        y = librosa.effects.time_stretch(y, rate=0.8)\n",
    "    elif current_emotion == 'angry':\n",
    "        y = librosa.effects.time_stretch(y, rate=1.3)\n",
    "        y = librosa.effects.pitch_shift(y, sr=sr, n_steps=5)\n",
    "\n",
    "    # Save and play the modified song using documentation from: https://www.pygame.org/docs/ref/music.html\n",
    "        \n",
    "    sf.write('modified_song.wav', y, sr)\n",
    "    pygame.mixer.music.fadeout(1000)  # Fade out current music\n",
    "    pygame.mixer.music.load('modified_song.wav')  # Load new song\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "current_emotion = 'neutral'\n",
    "prev_emotion = 'neutral'\n",
    "\n",
    "# debugged with gpt, original code was adapted from; https://roboflow.com/use-opencv/read-video-streams-with-cv2-videocapture \n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw bounding box around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
    "\n",
    "        # Get the landmarks\n",
    "        dlib_rect = dlib.rectangle(x, y, x + w, y + h)\n",
    "        landmarks = shape_predictor(gray, dlib_rect)\n",
    "\n",
    "        # Extract facial features for Random Forest\n",
    "        left_eye = np.array([landmarks.part(36).x, landmarks.part(36).y])\n",
    "        right_eye = np.array([landmarks.part(45).x, landmarks.part(45).y])\n",
    "        nose_tip = np.array([landmarks.part(30).x, landmarks.part(30).y])\n",
    "        mouth_left = np.array([landmarks.part(48).x, landmarks.part(48).y])\n",
    "        mouth_right = np.array([landmarks.part(54).x, landmarks.part(54).y])\n",
    "        chin = np.array([landmarks.part(8).x, landmarks.part(8).y])\n",
    "        \n",
    "        # Calculate distances (features used during training)\n",
    "        eye_distance = np.linalg.norm(left_eye - right_eye)\n",
    "        nose_to_mouth = np.linalg.norm(nose_tip - (mouth_left + mouth_right) / 2)\n",
    "        nose_to_left_eye = np.linalg.norm(nose_tip - left_eye)\n",
    "        nose_to_right_eye = np.linalg.norm(nose_tip - right_eye)\n",
    "        mouth_width = np.linalg.norm(mouth_left - mouth_right)\n",
    "        nose_to_chin = np.linalg.norm(nose_tip - chin)\n",
    "        left_eye_to_chin = np.linalg.norm(left_eye - chin)\n",
    "        right_eye_to_chin = np.linalg.norm(right_eye - chin)\n",
    "        mouth_to_chin = np.linalg.norm((mouth_left + mouth_right) / 2 - chin)\n",
    "\n",
    "        # Prepare the feature vector for Random Forest (9 features used during training)\n",
    "        features = np.array([[eye_distance, nose_to_mouth, nose_to_left_eye, nose_to_right_eye, mouth_width, nose_to_chin, left_eye_to_chin, right_eye_to_chin, mouth_to_chin]])\n",
    "        features_scaled = scaler.transform(features)\n",
    "\n",
    "        # Predict the emotion with Random Forest\n",
    "        rf_emotion_label = rf_model.predict(features_scaled)[0]\n",
    "        rf_emotion_text = emotion_map.get(rf_emotion_label, \"Unknown\")\n",
    "\n",
    "        # Prepare the image for CNN (150x150 as per your earlier configuration)\n",
    "        face_img = cv2.resize(frame[y:y+h, x:x+w], (150, 150))\n",
    "        face_img = face_img / 255.0  # Normalize to [0, 1]\n",
    "        face_img = np.expand_dims(face_img, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Predict the emotion with CNN\n",
    "        cnn_predictions = cnn_model.predict(face_img)\n",
    "        cnn_emotion_label = np.argmax(cnn_predictions, axis=1)[0]\n",
    "        cnn_emotion_text = emotion_map.get(cnn_emotion_label, \"Unknown\")\n",
    "\n",
    "        # Combine the predictions (simple voting mechanism)\n",
    "        if rf_emotion_text == cnn_emotion_text:\n",
    "            final_emotion_text = rf_emotion_text\n",
    "        else:\n",
    "            # Prioritize CNN prediction if different from Random Forest\n",
    "            final_emotion_text = cnn_emotion_text\n",
    "\n",
    "        # Display the emotion on the frame\n",
    "        cv2.putText(frame, final_emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "        # Add the predicted emotion to the buffer\n",
    "        emotion_buffer.append(final_emotion_text)\n",
    "\n",
    "        # Get the most common emotion in the buffer\n",
    "        common_emotion = max(set(emotion_buffer), key=emotion_buffer.count)\n",
    "\n",
    "        # Change the music only if the emotion has stabilized (debugged with gpt)\n",
    "        if common_emotion != current_emotion and emotion_buffer.count(common_emotion) > 15:\n",
    "            current_emotion = common_emotion\n",
    "            change_music_based_on_emotion(current_emotion)\n",
    "\n",
    "    # Display the frame with face and emotion overlay\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
